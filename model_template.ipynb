{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    '<START>', '<END>', '<PAD>', 'measure', 'note', 'pitch', 'step', 'alter',\n",
    "    'octave', 'duration', 'type', 'rest', 'dot', 'staff', 'notations', 'slur',\n",
    "    'ff', 'f', 'mf', 'mp', 'p', 'pp', 'backup', 'chord'] + list('ABCDEFG') \\\n",
    "+ ['-1'] + list('0123456789') + ['10', '11', '12', '13', '14', '15', '16'] + ['}'] \\\n",
    "+ ['whole', 'half', 'quarter', 'eighth', '16th']\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(tokens)}\n",
    "ix_to_word = {str(i): word for i, word in enumerate(tokens)}\n",
    "len_lexicon = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'storage/samples/'\n",
    "lstm_hidden_size = 256\n",
    "fc1_output_size = 256\n",
    "seq_len = 64\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_signature_layer(measure_length, height=224, width=224):\n",
    "    # measure length is 0 for 4/4 and 1 for 3/4\n",
    "    x = np.zeros((height, width)).astype(np.uint8)\n",
    "    if measure_length == 12:\n",
    "        x[:int(height/2)] += 255\n",
    "    if measure_length == 16:\n",
    "        x[int(height/2):] += 255\n",
    "    return x\n",
    "\n",
    "def get_key_signature_layer(key_number, height=224, width=224):\n",
    "    # key number is between -7 and 7 inclusive\n",
    "    x = np.zeros((height, width)).astype(np.uint8)\n",
    "    splits = np.array_split(x, 15)\n",
    "    splits[key_number+7] += 255\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, path, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.images = np.load(path + 'images.npy')\n",
    "        self.measure_lengths = np.load(path + 'measure_lengths.npy')\n",
    "        self.key_numbers = np.load(path + 'key_numbers.npy')\n",
    "        with open(path + 'pc_data.json') as f:\n",
    "            self.pc_data = json.load(f)\n",
    "        self.sequences = []\n",
    "        self.image_indices = []\n",
    "        for i, pc in enumerate(self.pc_data):\n",
    "            if len(pc) < seq_len+1:\n",
    "                pc = pc + ['<PAD>']*(seq_len+1-len(pc))\n",
    "            pc_as_ix = np.array([int(word_to_ix[word]) for word in pc])\n",
    "            for j in range(len(pc)-seq_len):\n",
    "                self.sequences.append(pc_as_ix[j:j+seq_len+1])\n",
    "                self.image_indices.append(i)\n",
    "        self.image_indices = np.array(self.image_indices)\n",
    "        self.sequences = np.array(self.sequences)\n",
    "            \n",
    "    def get_batch(self, batch_size, val=False):\n",
    "        validation_partition = int(len(self.sequences)*0.9)\n",
    "        if not val:\n",
    "            sequence_batch_indices = np.random.choice(len(self.sequences[:validation_partition]), size=batch_size)\n",
    "        else:\n",
    "            sequence_batch_indices = np.random.choice(len(self.sequences[validation_partition:]), size=batch_size)\n",
    "        image_batch_indices = self.image_indices[sequence_batch_indices]\n",
    "        raw_image_batch = self.images[image_batch_indices].reshape(-1, 1, 224, 224)\n",
    "        measure_lengths_batch = self.measure_lengths[image_batch_indices]\n",
    "        key_numbers_batch = self.key_numbers[image_batch_indices]\n",
    "        measure_lengths_layers = []\n",
    "        key_numbers_layers = []\n",
    "        for i in range(batch_size):\n",
    "            measure_length = measure_lengths_batch[i]\n",
    "            key_number = key_numbers_batch[i]\n",
    "            measure_lengths_layers.append(get_time_signature_layer(measure_length))\n",
    "            key_numbers_layers.append(get_key_signature_layer(key_number))\n",
    "        measure_lengths_layers = np.array(measure_lengths_layers).reshape(-1, 1, 224, 224)\n",
    "        key_numbers_layers = np.array(key_numbers_layers).reshape(-1, 1, 224, 224)\n",
    "        image_batch = np.concatenate([raw_image_batch, measure_lengths_layers, key_numbers_layers], axis=1)\n",
    "        sequence_batch = self.sequences[sequence_batch_indices]\n",
    "        image_batch = torch.Tensor(image_batch).type(torch.float).to(device)\n",
    "        sequence_batch = torch.Tensor(sequence_batch).type(torch.long).to(device)\n",
    "        return image_batch, sequence_batch, image_batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(n):\n",
    "    all_datasets = os.listdir(dataset_dir)\n",
    "    dataset_sample = []\n",
    "    for _ in range(n):\n",
    "        i = np.random.randint(len(all_datasets))\n",
    "        filename = all_datasets[i]\n",
    "        dataset = Dataset(dataset_dir + filename + '/', seq_len)\n",
    "        dataset_sample.append(dataset)\n",
    "    return dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSubunit(nn.Module):\n",
    "    def __init__(self, input_size, output_size, filter_size, stride, padding, dropout):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_size, output_size, filter_size, stride=stride, padding=padding)\n",
    "        self.dp = nn.Dropout2d(p=dropout)\n",
    "        self.bn = nn.BatchNorm2d(output_size)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.sequential = nn.Sequential(self.conv, self.dp, self.bn, self.relu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "    \n",
    "class LargeConvUnit(nn.Module):\n",
    "    def __init__(self, input_size, output_size, filter_size, stride, padding, dropout):\n",
    "        super().__init__()\n",
    "        self.subunit1 = ConvSubunit(input_size, output_size, filter_size, 1, padding, dropout)\n",
    "        self.subunit2 = ConvSubunit(output_size, output_size, filter_size, 1, padding, dropout)\n",
    "        self.subunit3 = ConvSubunit(output_size, output_size, filter_size, 1, padding, dropout)\n",
    "        self.subunit4 = ConvSubunit(output_size, output_size, filter_size, stride, padding, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.subunit1(x)\n",
    "        cache = x\n",
    "        x = self.subunit2(x)\n",
    "        x = self.subunit3(x)\n",
    "        x = x + cache\n",
    "        x = self.subunit4(x)\n",
    "        return x\n",
    "    \n",
    "class SmallConvUnit(nn.Module):\n",
    "    def __init__(self, input_size, output_size, filter_size, stride, padding, dropout):\n",
    "        super().__init__()\n",
    "        self.subunit1 = ConvSubunit(input_size, output_size, filter_size, 1, padding, dropout)\n",
    "        self.subunit2 = ConvSubunit(output_size, output_size, filter_size, stride, padding, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.subunit1(x)\n",
    "        x = self.subunit2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(SmallConvUnit(3, 32, 3, 2, 1, 0.1), # (224, 224) --> (112, 112)\n",
    "                                 SmallConvUnit(32, 64, 3, 2, 1, 0.1), # (112, 112) --> (56, 56)\n",
    "                                 SmallConvUnit(64, 128, 3, 4, 1, 0.1), # (56, 56) --> (14, 14)\n",
    "                                 SmallConvUnit(128, 256, 3, 7, 1, 0.1)) # (14, 14) --> (2, 2)\n",
    "        self.fc = nn.Linear(1024, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "class LargeCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(LargeConvUnit(3, 32, 3, 2, 1, 0.1), # (224, 224) --> (112, 112)\n",
    "                                 LargeConvUnit(32, 64, 3, 2, 1, 0.1), # (112, 112) --> (56, 56)\n",
    "                                 LargeConvUnit(64, 128, 3, 4, 1, 0.1), # (56, 56) --> (14, 14)\n",
    "                                 LargeConvUnit(128, 256, 3, 7, 1, 0.1)) # (14, 14) --> (2, 2)\n",
    "        self.fc = nn.Linear(1024, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "class SqueezeCNN(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        self.cnn = torchvision.models.squeezenet1_0(pretrained=True)\n",
    "        self.cnn.classifier[1] = nn.Conv2d(512, output_size, kernel_size=(1,1), stride=(1,1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, save_dir, cnn, len_lexicon, lstm_hidden_size, fc1_output_size, device, num_directions=2):\n",
    "        super().__init__()\n",
    "        self.save_dir = save_dir\n",
    "        self.len_lexicon = len_lexicon\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        self.fc1_output_size = fc1_output_size\n",
    "        self.num_directions = num_directions\n",
    "        self.bidirectional = (num_directions==2)\n",
    "        self.cnn = cnn\n",
    "        self.embed = nn.Embedding(num_embeddings=self.len_lexicon, embedding_dim=5)\n",
    "        self.lstm1 = nn.LSTM(input_size=5,\n",
    "                             hidden_size=self.lstm_hidden_size,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.3,\n",
    "                             bidirectional=self.bidirectional)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.fc1_output_size+self.num_directions*self.lstm_hidden_size,\n",
    "                             hidden_size=self.lstm_hidden_size,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.3,\n",
    "                             bidirectional=self.bidirectional)\n",
    "        self.fc2 = nn.Linear(self.num_directions*self.lstm_hidden_size, self.len_lexicon)\n",
    "        \n",
    "    def forward(self, image_input, sequence_input, internal1=None, internal2=None):\n",
    "        bs = image_input.shape[0]\n",
    "        sl = sequence_input.shape[1]\n",
    "        if internal1:\n",
    "            h1, c1 = internal1\n",
    "        else:\n",
    "            h1 = torch.zeros(2*self.num_directions, bs, self.lstm_hidden_size).to(device)\n",
    "            c1 = torch.zeros(2*self.num_directions, bs, self.lstm_hidden_size).to(device)\n",
    "        if internal2:\n",
    "            h2, c2 = internal2\n",
    "        else:\n",
    "            h2 = torch.zeros(2*self.num_directions, bs, self.lstm_hidden_size).to(device)\n",
    "            c2 = torch.zeros(2*self.num_directions, bs, self.lstm_hidden_size).to(device)\n",
    "        image_output = self.cnn(image_input)\n",
    "        image_output = image_output.repeat(1, sl).view(bs, sl, self.fc1_output_size)\n",
    "        sequence_output, (h1, c1) = self.lstm1(self.embed(sequence_input), (h1, c1))\n",
    "        concatenated = torch.cat([image_output, sequence_output], 2)\n",
    "        lstm2_out, (h2, c2) = self.lstm2(concatenated, (h2, c2))\n",
    "        out = self.fc2(lstm2_out)\n",
    "        return out, (h1, c1), (h2, c2)\n",
    "    \n",
    "    def fit(self, iterations, batch_size, optimizer, loss_fn, print_every=100, save_every=5000, train_time=0, past_iterations=0):\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        time_checkpoint = time.time()\n",
    "        for i in range(iterations):\n",
    "            self.train()\n",
    "            if i % 500 == 0:\n",
    "                dataset = get_datasets(1)[0]\n",
    "            arr, seq, _ = dataset.get_batch(batch_size)\n",
    "            seq1 = seq[:, :-1]\n",
    "            seq2 = seq[:, 1:]\n",
    "            out, _, _ = self.forward(arr, seq1)\n",
    "            out = out.view(-1, self.len_lexicon)\n",
    "            targets = seq2.reshape(-1)\n",
    "            loss = loss_fn(out, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_time += time.time() - time_checkpoint\n",
    "            time_checkpoint = time.time()\n",
    "            if i % print_every == 0:\n",
    "                arr, _, image_batch_indices = dataset.get_batch(1, val=True)\n",
    "                pc = dataset.pc_data[image_batch_indices[0]]\n",
    "#                 pc = ' '.join([ix_to_word[str(ix)] for ix in pc])\n",
    "                pc = ' '.join(pc)\n",
    "                pred_seq = self.predict(arr)\n",
    "                pred_seq = ' '.join(pred_seq)\n",
    "                pred_seq2 = self.predict_stochastic(arr)\n",
    "                pred_seq2 = ' '.join(pred_seq2)\n",
    "                with open(f'{self.save_dir}log.txt', 'a+') as f:\n",
    "                    info_string = f\"\"\"\n",
    "                    ----\n",
    "                    iteration: {i}\n",
    "                    time elapsed: {train_time}\n",
    "                    loss: {loss}\n",
    "                    ----\n",
    "                    pred1: {pred_seq}\n",
    "                    ----\n",
    "                    pred2: {pred_seq2}\n",
    "                    ----\n",
    "                    true:  {pc}\n",
    "                    ----\n",
    "\n",
    "\n",
    "\n",
    "                    \"\"\".replace('    ', '')\n",
    "                    print(info_string)\n",
    "                    f.write(info_string)\n",
    "            if i % save_every == 0 and i != 0:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.92\n",
    "                torch.save(self.state_dict(), f'{self.save_dir}checkpoint_iteration_{past_iterations+i}.pt')\n",
    "                with open(f'{self.save_dir}training_info.json', 'w+') as f:\n",
    "                    json.dump({'train_time': train_time, 'past_iterations': past_iterations+i}, f)\n",
    "                    \n",
    "    def resume_fit(self, iterations, optimizer, loss_fn, print_every=100, save_every=5000):\n",
    "        with open(f'{self.save_dir}training_info.json', 'w+') as f:\n",
    "            training_info = json.load(f)\n",
    "        train_time = training_info['train_time']\n",
    "        past_iterations = training_info['past_iterations']\n",
    "        checkpoint = torch.load(f'{self.save_dir}checkpoint_iteration_{past_iterations}.pt')\n",
    "        self.load_state_dict(checkpoint)\n",
    "        self.fit(iterations, optimizer, loss_fn, print_every=print_every, save_every=save_every, train_time=train_time, past_iterations=past_iterations)\n",
    "                \n",
    "             \n",
    "    def predict(self, arr):\n",
    "        self.eval()    \n",
    "        with torch.no_grad():\n",
    "            arr = arr.view(1,3, 224, 224)\n",
    "            output_sequence = ['<START>']\n",
    "            h1 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            c1 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            h2 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            c2 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            while output_sequence[-1] != '<END>' and len(output_sequence)<400:\n",
    "                sequence_input = torch.Tensor([word_to_ix[output_sequence[-1]]]).type(torch.long).view(1, 1).to(device)\n",
    "                out, (h1, c1), (h2, c2) = self.forward(arr, sequence_input, (h1, c1), (h2, c2))\n",
    "                _, sequence_input = out[0, 0, :].max(0)\n",
    "                output_sequence.append(ix_to_word[str(sequence_input.item())])\n",
    "        self.train()\n",
    "        return output_sequence\n",
    "    \n",
    "    def predict_stochastic(self, arr):\n",
    "        self.eval()    \n",
    "        with torch.no_grad():\n",
    "            arr = arr.view(1,3, 224, 224)\n",
    "            output_sequence = ['<START>']\n",
    "            h1 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            c1 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            h2 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            c2 = torch.zeros(2*self.num_directions, 1, self.lstm_hidden_size).to(device)\n",
    "            while output_sequence[-1] != '<END>' and len(output_sequence)<400:\n",
    "                sequence_input = torch.Tensor([word_to_ix[output_sequence[-1]]]).type(torch.long).view(1, 1).to(device)\n",
    "                out, (h1, c1), (h2, c2) = self.forward(arr, sequence_input, (h1, c1), (h2, c2))\n",
    "                log_probs = out[0, 0, :].cpu().numpy()\n",
    "                probs = np.exp(log_probs) / np.exp(log_probs).sum()\n",
    "                predicted_ix = np.random.choice(len_lexicon, p=probs)\n",
    "                output_sequence.append(ix_to_word[str(predicted_ix)])\n",
    "        self.train()\n",
    "        return output_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_cnn = SmallCNN(fc1_output_size)\n",
    "large_cnn = LargeCNN(fc1_output_size)\n",
    "squeeze_cnn = SqueezeCNN(fc1_output_size)\n",
    "\n",
    "small_net = Net('small_model_lr3e-4', small_cnn, len_lexicon, lstm_hidden_size, fc1_output_size, device)\n",
    "large_net = Net('large_model_lr3e-4', large_cnn, len_lexicon, lstm_hidden_size, fc1_output_size, device)\n",
    "squeeze_net = Net('squeeze_model_lr3e-4', small_cnn, len_lexicon, lstm_hidden_size, fc1_output_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_optimizer = torch.optim.Adam(small_cnn.parameters(), lr=3e-4)\n",
    "small_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "large_optimizer = torch.optim.Adam(large_cnn.parameters(), lr=3e-4)\n",
    "large_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "squeeze_optimizer = torch.optim.Adam(squeeze_cnn.parameters(), lr=3e-4)\n",
    "suqeeze_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got dataset\n",
      "got batch\n",
      "passed through net\n",
      "tensor([[37, 49,  9,  ..., 13, 34, 49],\n",
      "        [33, 37, 49,  ..., 49, 13, 34],\n",
      "        [33, 49, 49,  ...,  5, 24, 33],\n",
      "        ...,\n",
      "        [36, 49, 10,  ..., 49,  4,  5],\n",
      "        [49,  9, 36,  ..., 34, 49, 49],\n",
      "        [36, 49, 10,  ..., 49,  4,  5]])\n",
      "computed loss\n",
      "backpropped\n",
      "updated weights\n",
      "\n",
      "----\n",
      "iteration: 0\n",
      "time elapsed: 8.17901062965393\n",
      "loss: 4.0084309577941895\n",
      "----\n",
      "pred1: <START> dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot dot\n",
      "----\n",
      "pred2: <START> f 6 type ff 6 F quarter 16 13 pitch f G dot measure step } F alter 4 1 2 slur D A <START> 5 C backup rest step 13 11 eighth G 2 C slur 2 B staff dot B 14 type 2 13 dot whole whole eighth alter eighth 6 mf 8 mf 8 3 B 10 4 5 notations 12 pitch staff 6 5 G 2 E 12 -1 <START> C 8 ff pp 1 8 16th rest 7 notations eighth notations 5 10 C 3 alter pp pp 6 backup half pp 3 A 3 f 2 quarter E duration 16th 3 <PAD> pp 12 half duration type 13 10 5 -1 15 mp p dot <PAD> 12 f <END>\n",
      "----\n",
      "true:  <START> measure note pitch E 0 5 } duration 4 } type quarter } staff 1 } } note pitch C 0 5 } duration 4 } type quarter } staff 1 } } backup duration 8 } } note pitch A 1 2 } duration 4 } type quarter } staff 2 } } note pitch A 0 2 } duration 4 } type quarter } staff 2 } } } <END>\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "got dataset\n",
      "got batch\n",
      "passed through net\n",
      "tensor([[ 5, 30, 33,  ..., 49, 49, 49],\n",
      "        [49,  4,  5,  ..., 13, 34, 49],\n",
      "        [ 9, 36, 49,  ..., 13, 34, 49],\n",
      "        ...,\n",
      "        [49,  9, 36,  ..., 49, 13, 34],\n",
      "        [ 9, 36, 49,  ..., 13, 34, 49],\n",
      "        [52, 49, 13,  ..., 26, 31, 35]])\n",
      "computed loss\n",
      "backpropped\n",
      "updated weights\n"
     ]
    }
   ],
   "source": [
    "small_net.fit(2, batch_size, small_optimizer, small_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
